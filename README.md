<p align="center">
  <strong>ğŸ”¥ HapticPhonix (CHD)</strong>
</p>

<p align="center">
  Real-time speech &amp; lip-reading learning platform with haptic feedback for the deaf and hard-of-hearing community.
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Next.js-16-black?logo=next.js" alt="Next.js" />
  <img src="https://img.shields.io/badge/React-19-blue?logo=react" alt="React" />
  <img src="https://img.shields.io/badge/FastAPI-0.118-009688?logo=fastapi" alt="FastAPI" />
  <img src="https://img.shields.io/badge/Snowflake-Cortex-29B5E8?logo=snowflake" alt="Snowflake" />
  <img src="https://img.shields.io/badge/MediaPipe-FaceMesh-4285F4?logo=google" alt="MediaPipe" />
  <img src="https://img.shields.io/badge/Gemini-Vision-8E75B2?logo=googlegemini" alt="Gemini" />
  <img src="https://img.shields.io/badge/ElevenLabs-STT%2FTTS-000?logo=elevenlabs" alt="ElevenLabs" />
</p>

---

## Overview

HapticPhonix helps people â€” especially those with hearing loss â€” learn speech through:

- **Lip reading** via MediaPipe Face Mesh + Google Gemini Vision
- **Haptic feedback** (phone vibrations) mapped to phonemes and speech patterns
- **Teacher / Student modes** for remote lesson delivery across devices
- **Speech-to-text** via ElevenLabs for real-time transcription and translation
- **AI coaching** via Snowflake Cortex multi-model LLMs for personalized progress feedback

The backend processes video and audio in real time, sending events to the frontend over WebSockets. The frontend drives the UI, connects to backend services, and triggers vibrations on mobile devices.

---

## System Architecture

```mermaid
graph TB
    subgraph Frontend["FRONTEND â€” Next.js 16 + React 19"]
        direction TB
        Landing["/ Landing Page"]
        Auth["(auth) Clerk Sign-In / Sign-Up"]

        subgraph Dashboard["Dashboard Routes â€” Protected by Clerk"]
            DashView["/dashboard â€” Main Viewer"]
            Teacher["/teacher â€” Teacher Controls"]
            Student["/student â€” Student Camera"]
            SpeechHaptic["/speech-haptic â€” Phone Haptic UI"]
            Lessons["/lessons â€” Lesson Browser"]
            Coach["/dashboard â†’ Coach Tab"]
        end

        subgraph FComponents["Components"]
            VideoPlayer["VideoPlayer + LipMeshOverlay"]
            LipReadingP["LipReadingPanel"]
            TranscriptionP["TranscriptionPanel"]
            PitchP["PitchAnalysisPanel"]
            PhonemeT["PhonemeTimeline"]
            WaveViz["WaveVisualizer3D â€” Three.js"]
            SnowflakeUI["SnowflakeCoaching Panel"]
        end

        subgraph FHooks["Hooks"]
            useSH["useSpeechHaptic"]
            usePA["usePitchAnalysis"]
            useLH["useLaryngealHaptics"]
            useMP["useMediaPipe"]
        end
    end

    subgraph Backend["BACKEND â€” FastAPI + Python"]
        direction TB
        subgraph WSManagers["WebSocket Managers"]
            ConnMgr["ConnectionManager â€” /ws/video"]
            ViewMgr["ViewerManager â€” /ws/viewer"]
            HapticMgr["HapticEventManager"]
            SpeechMgr["SpeechHapticManager â€” /ws/speech-haptic"]
        end

        subgraph Processors["Processing Engines"]
            MP["MediaPipe Processor â€” Face Mesh"]
            LipEngine["Lip Reading Engine â€” Gemini Vision"]
            PhonemeEng["Phoneme Engine â€” Lesson Playback"]
            SpeechPipe["Speech-Haptic Pipeline â€” Mic to STT"]
            TTS["TTS Service â€” Voice Cues"]
            SnowCoach["Snowflake Coach â€” Cortex LLM"]
        end
    end

    subgraph External["EXTERNAL SERVICES"]
        MediaPipeModel["MediaPipe face_landmarker"]
        Gemini["Google Gemini API"]
        ElevenLabs["ElevenLabs API"]
        Snowflake["Snowflake Cortex"]
        ClerkAuth["Clerk Auth"]
    end

    Student -->|"ws/video â€” send frames"| ConnMgr
    DashView -->|"ws/viewer â€” receive events"| ViewMgr
    Teacher -->|"ws/viewer â€” receive events"| ViewMgr
    SpeechHaptic -->|"ws/speech-haptic â€” receive haptics"| SpeechMgr
    Coach -->|"REST /api/coaching-*"| SnowCoach

    ConnMgr --> MP
    MP --> LipEngine
    MP --> PhonemeEng
    PhonemeEng --> HapticMgr
    HapticMgr --> ViewMgr
    SpeechPipe --> SpeechMgr

    MP -.-> MediaPipeModel
    LipEngine -.-> Gemini
    SpeechPipe -.-> ElevenLabs
    TTS -.-> ElevenLabs
    SnowCoach -.-> Snowflake
    Auth -.-> ClerkAuth
```

---

## Tech Stack

| Layer | Technologies | How We Use It |
|---|---|---|
| **Frontend** | Next.js 16, React 19, TypeScript 5, Tailwind CSS 4 | App Router with protected dashboard routes, server/client components, responsive UI |
| **3D & Animation** | Three.js, React Three Fiber, React Three Drei, Framer Motion, GSAP | `WaveVisualizer3D` renders real-time haptic waveforms; Framer Motion drives dashboard animations |
| **Authentication** | Clerk (`@clerk/nextjs` 6) | Sign-in/sign-up flows, middleware-enforced route protection for `/dashboard`, `/teacher`, `/student`, `/lessons` |
| **Backend** | FastAPI 0.118, Uvicorn, Python 3.11+ | REST endpoints + WebSocket handlers for real-time bidirectional communication |
| **Computer Vision** | MediaPipe Face Mesh 0.10, OpenCV | `MediaPipeProcessor` extracts 468 face landmarks, lip bounding boxes, and mouth-state from video frames |
| **Lip Reading AI** | Google Gemini Vision (`google-genai`) | Cropped lip-region frames are buffered and sent to Gemini every ~3s for text/phoneme recognition |
| **Speech-to-Text** | ElevenLabs STT (`scribe_v2`) | Real-time mic capture â†’ ElevenLabs transcription â†’ chunked text + haptic intensity mapping |
| **Text-to-Speech** | ElevenLabs TTS | Generates voice cues ("Listening started") broadcast to connected phones |
| **Translation** | Google Gemini | Transcribed speech is sent to Gemini for multi-language translation |
| **AI Coaching** | Snowflake Cortex (Mistral Large, Llama 3 70B, Mixtral 8Ã—7B) | Personalized coaching feedback after practice sessions, phoneme tips, session trend analysis |
| **Audio Analysis** | PyAudio, Web Audio API, Praat-Parselmouth | Backend mic capture; frontend pitch analysis via `usePitchAnalysis` for laryngeal haptic feedback |
| **ML / Deep Learning** | TensorFlow 2.20, PyTorch 2.6, Transformers | Model inference support for vision and speech pipelines |
| **WebSockets** | FastAPI WebSocket, `websockets` 15 | Four WS endpoints managing video, viewer, haptic, and speech-haptic connections |
| **Icons & UI** | Lucide React | Consistent iconography across the dashboard |

---

## Project Structure

### Frontend

```
frontend/
â”œâ”€â”€ app/                              # Next.js App Router
â”‚   â”œâ”€â”€ page.tsx                      # Landing page (redirects signed-in users)
â”‚   â”œâ”€â”€ layout.tsx                    # Root layout with ClerkProvider
â”‚   â”œâ”€â”€ globals.css                   # Tailwind v4 theme (copper/dark palette)
â”‚   â”œâ”€â”€ (auth)/                       # Public auth routes
â”‚   â”‚   â”œâ”€â”€ signIn/[[...sign-in]]/page.tsx
â”‚   â”‚   â””â”€â”€ signUp/[[...sign-up]]/page.tsx
â”‚   â”œâ”€â”€ (dashboard)/                  # Protected routes (Clerk middleware)
â”‚   â”‚   â”œâ”€â”€ dashboard/page.tsx        # Main viewer â€” video, lip reading, haptics, coaching
â”‚   â”‚   â”œâ”€â”€ teacher/page.tsx          # Teacher controls & lesson playback
â”‚   â”‚   â”œâ”€â”€ student/page.tsx          # Student camera + local pitch haptics
â”‚   â”‚   â”œâ”€â”€ speech-haptic/page.tsx    # Phone-only speech â†’ haptic UI
â”‚   â”‚   â””â”€â”€ lessons/page.tsx          # Lesson browser
â”‚   â””â”€â”€ api/                          # Next.js API routes
â”‚       â”œâ”€â”€ lessons/route.ts
â”‚       â”œâ”€â”€ socket/route.ts
â”‚       â””â”€â”€ webhooks/clerk/route.ts
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ VideoPlayer.tsx               # Camera frame display + landmark overlay
â”‚   â”œâ”€â”€ LipMeshOverlay.tsx            # Lip mesh wireframe renderer
â”‚   â”œâ”€â”€ LipReadingPanel.tsx           # Gemini lip reading results
â”‚   â”œâ”€â”€ ConnectionStatus.tsx          # WS connection indicator
â”‚   â”œâ”€â”€ PitchAnalysisPanel.tsx        # Real-time pitch visualization
â”‚   â”œâ”€â”€ RecordingControls.tsx         # Record & replay sessions
â”‚   â”œâ”€â”€ TranscriptionPanel.tsx        # ElevenLabs STT + Gemini translation
â”‚   â”œâ”€â”€ PhonemeTimeline.tsx           # Phoneme sequence display
â”‚   â”œâ”€â”€ SnowflakeCoaching.tsx         # AI coaching panel (Snowflake Cortex)
â”‚   â”œâ”€â”€ WaveVisualizer3D.tsx          # Three.js haptic wave visualization
â”‚   â”œâ”€â”€ HapticFeedback.tsx            # Vibration pattern display
â”‚   â”œâ”€â”€ LaryngealHapticsPanel.tsx     # Laryngeal feedback controls
â”‚   â”œâ”€â”€ Audiogram.tsx                 # Hearing profile
â”‚   â”œâ”€â”€ ParticleBackground.tsx        # Landing page particles
â”‚   â””â”€â”€ ui/navbar.tsx                 # Navigation bar
â”œâ”€â”€ hooks/
â”‚   â”œâ”€â”€ useSpeechHaptic.ts            # Speech-haptic WS + pipeline API
â”‚   â”œâ”€â”€ usePitchAnalysis.ts           # Local mic â†’ pitch extraction
â”‚   â”œâ”€â”€ useLaryngealHaptics.ts        # Pitch â†’ haptic vibration patterns
â”‚   â”œâ”€â”€ useMediaPipe.ts               # Client-side MediaPipe
â”‚   â”œâ”€â”€ useSocket.ts                  # Generic WebSocket hook
â”‚   â””â”€â”€ useVibration.ts               # Vibration API wrapper
â”œâ”€â”€ lib/
â”‚   â”œâ”€â”€ storage.ts                    # Session recording storage
â”‚   â”œâ”€â”€ pitchAnalysis.ts              # Pitch detection utilities
â”‚   â”œâ”€â”€ laryngealHaptics.ts           # Haptic mapping logic
â”‚   â”œâ”€â”€ mediapipe.ts                  # MediaPipe config
â”‚   â”œâ”€â”€ haptics.ts                    # Haptic pattern helpers
â”‚   â””â”€â”€ socket.ts                     # Socket configuration
â”œâ”€â”€ types/index.ts                    # Shared TypeScript types
â”œâ”€â”€ middleware.ts                     # Clerk route protection
â””â”€â”€ public/                           # Static assets
```

### Backend

```
backend/
â”œâ”€â”€ main.py                           # FastAPI app â€” all routes & WS handlers
â”œâ”€â”€ websocket_server.py               # Connection managers (5 managers)
â”œâ”€â”€ mediapipe_processor.py            # MediaPipe Face Mesh processing
â”œâ”€â”€ lip_reading.py                    # Gemini Vision lip reading engine
â”œâ”€â”€ phoneme_engine.py                 # Lesson playback & phoneme timing
â”œâ”€â”€ speech_haptic_pipeline.py         # Mic â†’ ElevenLabs STT â†’ haptic output
â”œâ”€â”€ tts_service.py                    # ElevenLabs TTS for voice cues
â”œâ”€â”€ snowflake_coach.py                # Snowflake Cortex AI coaching
â”œâ”€â”€ haptic_patterns.py                # Vibration pattern definitions
â”œâ”€â”€ config.py                         # App configuration
â”œâ”€â”€ face_landmarker.task              # MediaPipe model file
â”œâ”€â”€ requirements.txt                  # Python dependencies
â”œâ”€â”€ .env.local                        # Environment variables
â””â”€â”€ lessons/
    â””â”€â”€ sample_lesson.json            # Phoneme sequence with timing data
```

---

## Data Flows

### 1. Video Pipeline (Student â†’ Backend â†’ Viewers)

```
Student Phone Camera
    â”‚ base64 frames via /ws/video
    â–¼
â”Œâ”€ Backend â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MediaPipe Face Mesh                  â”‚
â”‚   â†’ 468 landmarks + lip bounding box  â”‚
â”‚   â†’ mouth openness + state            â”‚
â”‚                                       â”‚
â”‚  Lip Reading Engine                   â”‚
â”‚   â†’ crop lip region â†’ buffer frames   â”‚
â”‚   â†’ Gemini Vision every ~3s           â”‚
â”‚   â†’ text, confidence, phonemes        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚ broadcast via /ws/viewer
    â–¼
Dashboard / Teacher (frames + landmarks + lip reading)
```

### 2. Lesson / Phoneme Pipeline

```
JSON Lesson File (phonemes + timing)
    â”‚ POST /lessons/load/{name}
    â–¼
PhonemeEngine â†’ background playback loop
    â”‚ phoneme becomes active
    â–¼
HapticEventManager â†’ builds vibration pattern
    â”‚ broadcast to /ws/viewer
    â–¼
All clients call navigator.vibrate(pattern)
```

### 3. Speech-Haptic Pipeline (Teacher Voice â†’ Student Phone)

```
Teacher Mic
    â”‚ POST /api/speech-haptic/start
    â–¼
PyAudio capture â†’ ElevenLabs STT
    â”‚ text chunks + RMS intensity
    â–¼
SpeechHapticPipeline
    â”‚ maps intensity â†’ haptic pattern
    â”‚ broadcast via /ws/speech-haptic
    â–¼
Student Phone vibrates + shows transcript
```

There is also a fast "volumetric" path that triggers haptics directly from RMS **before** STT completes, ensuring near-zero-latency feedback.

### 4. AI Coaching Pipeline (Snowflake Cortex)

```
Practice Session Data (phonemes, scores, struggles)
    â”‚ POST /api/coaching-feedback
    â–¼
Snowflake Coach
    â”‚ builds context-aware prompt
    â”‚ calls SNOWFLAKE.CORTEX.COMPLETE()
    â”‚ model: mistral-large / llama3-70b / mixtral-8x7b
    â–¼
Personalized feedback â†’ Dashboard Coach Tab
    â”œâ”€â”€ Encouragement
    â”œâ”€â”€ Focus areas
    â””â”€â”€ Next steps
```

### 5. Transcription & Translation

```
Browser mic â†’ RecordingControls
    â”‚ audio blob
    â–¼
POST /api/transcribe â†’ ElevenLabs STT
    â”‚ transcript text
    â–¼
POST /api/translate â†’ Gemini
    â”‚ translated text
    â–¼
Dashboard displays both
```

---

## Routes & Roles

| Route | Purpose | Protection |
|---|---|---|
| `/` | Landing page; redirects signed-in users to `/dashboard` | Public |
| `/dashboard` | Main viewer: live video, lip reading, haptic log, phoneme timeline, AI coaching | Clerk |
| `/teacher` | Teacher controls, lesson playback, transcription, haptic triggers | Clerk |
| `/student` | Student phone: camera feed, local pitch â†’ haptics, remote speech-haptic | Clerk |
| `/speech-haptic` | Phone-only UI for receiving speech â†’ haptic events | Clerk |
| `/lessons` | Lesson browser and content | Clerk |

---

## WebSocket Endpoints

| Endpoint | Client | Direction | Purpose |
|---|---|---|---|
| `/ws/video` | Student phone | Phone â†’ Backend | Send camera frames, receive processed frames back |
| `/ws/viewer` | Dashboard / Teacher | Backend â†’ Client | Receive frames, haptic events, lip reading results, speech analysis |
| `/ws/speech-haptic` | Student phone | Backend â†’ Phone | Receive speech-haptic events and TTS cues |

---

## REST API Endpoints

| Endpoint | Method | Purpose |
|---|---|---|
| `/` | GET | Server status and connection counts |
| `/health` | GET | Health check |
| `/lessons` | GET | List available lesson JSON files |
| `/lessons/load/{name}` | POST | Load lesson into phoneme engine |
| `/playback/start` | POST | Start lesson playback |
| `/playback/pause` | POST | Pause lesson playback |
| `/playback/resume` | POST | Resume lesson playback |
| `/playback/stop` | POST | Stop lesson playback |
| `/playback/status` | GET | Current playback status |
| `/haptic/test` | POST | Send test haptic to all devices |
| `/api/lip-read` | POST | On-demand lip reading (single frame) |
| `/api/lip-read/history` | GET | Lip reading history |
| `/api/transcribe` | POST | ElevenLabs transcription |
| `/api/translate` | POST | Gemini translation |
| `/api/speech-haptic/start` | POST | Start speech-haptic pipeline |
| `/api/speech-haptic/stop` | POST | Stop pipeline |
| `/api/speech-haptic/status` | GET | Pipeline status |
| `/api/coaching-feedback` | POST | Snowflake Cortex coaching feedback |
| `/api/phoneme-tip/{phoneme}` | GET | Quick phoneme pronunciation tip |
| `/api/coaching/status` | GET | Coaching engine status & available models |
| `/api/coaching/trends` | GET | Multi-session trend analysis |

---

## Connection Managers

Defined in `websocket_server.py`:

| Manager | Role |
|---|---|
| **ConnectionManager** | Tracks phone connections on `/ws/video` |
| **ViewerManager** | Tracks dashboard/teacher connections on `/ws/viewer`; broadcasts frames, haptics, lip reading |
| **HapticEventManager** | Builds vibration patterns from phoneme events and triggers broadcasts |
| **SpeechAnalysisManager** | Handles speech analysis results and broadcasts to viewers |
| **SpeechHapticConnectionManager** | Tracks phones on `/ws/speech-haptic` and broadcasts speech-haptic events |

---

## Student Page: Dual Haptic Sources

The Student page combines two independent haptic paths:

| Source | How It Works |
|---|---|
| **Local pitch â†’ haptics** | Device mic â†’ Web Audio API pitch extraction (`usePitchAnalysis`) â†’ mapped to haptic patterns (`useLaryngealHaptics`) for self-monitoring |
| **Remote speech-haptic** | `/ws/speech-haptic` â†’ receives teacher speech as transcript chunks + vibration patterns (`useSpeechHaptic`) |

When both are active, **local pitch-based haptics take precedence** while the student is speaking; remote speech-haptic is used when the teacher is speaking.

---

## Environment Variables

### Frontend (`frontend/.env`)

| Variable | Purpose |
|---|---|
| `NEXT_PUBLIC_CLERK_PUBLISHABLE_KEY` | Clerk public key |
| `CLERK_SECRET_KEY` | Clerk server-side key |
| `NEXT_PUBLIC_CLERK_SIGN_IN_URL` | Sign-in route |
| `NEXT_PUBLIC_CLERK_SIGN_UP_URL` | Sign-up route |
| `NEXT_PUBLIC_CLERK_AFTER_SIGN_IN_URL` | Redirect after sign-in |
| `NEXT_PUBLIC_CLERK_AFTER_SIGN_UP_URL` | Redirect after sign-up |
| `NEXT_PUBLIC_API_URL` | Backend URL (default `http://localhost:8000`) |
| `NEXT_PUBLIC_WS_URL` | WebSocket URL |
| `GEMINI_API_KEY` | Google Gemini API key |
| `ELEVENLABS_API_KEY` | ElevenLabs API key |

### Backend (`backend/.env.local`)

| Variable | Purpose |
|---|---|
| `PORT` | Server port (default `8000`) |
| `CORS_ORIGIN` | Allowed CORS origin |
| `GEMINI_API_KEY` | Google Gemini â€” lip reading + translation |
| `ELEVENLABS_API_KEY` | ElevenLabs â€” STT and TTS |
| `SNOWFLAKE_ACCOUNT` | Snowflake account identifier |
| `SNOWFLAKE_USER` | Snowflake username |
| `SNOWFLAKE_PASSWORD` | Snowflake password |
| `SNOWFLAKE_MOCK_MODE` | Set `true` for demo without credentials |

---

## Getting Started

### Prerequisites

- **Node.js** 18+ and **npm**
- **Python** 3.11+
- API keys for Clerk, Gemini, and ElevenLabs

### Backend

```bash
cd backend
pip install -r requirements.txt

# Configure environment
cp .env.example .env.local
# Edit .env.local with your API keys

# Start the server
python main.py
# â†’ Runs on http://localhost:8000
```

### Frontend

```bash
cd frontend
npm install

# Configure environment
# Edit .env with your Clerk and API keys

# Start development server
npm run dev
# â†’ Runs on http://localhost:3000
```

### Quick Test

1. Open `http://localhost:3000` â†’ sign in via Clerk
2. Navigate to `/dashboard` â†’ see live video feed when a phone connects
3. Open `/student` on a phone (same network) â†’ camera starts, haptics enabled
4. Open `/speech-haptic` on another phone â†’ receives teacher speech as vibrations

---

## Summary

| What | How |
|---|---|
| **Frontend** | Next.js 16 app with Clerk auth, multiple roles (viewer, teacher, student, speech-haptic phone) |
| **Backend** | FastAPI with MediaPipe, Gemini, ElevenLabs, Snowflake, and 5 WebSocket managers |
| **Video** | Student phone â†’ `/ws/video` â†’ MediaPipe + lip reading â†’ `/ws/viewer` |
| **Lessons** | JSON phonemes â†’ PhonemeEngine â†’ haptics to all viewers and phones |
| **Speech-haptic** | Teacher mic â†’ ElevenLabs STT â†’ chunks â†’ `/ws/speech-haptic` â†’ phone vibration |
| **Local haptics** | Student mic â†’ Web Audio pitch â†’ laryngeal haptic patterns on device |
| **AI Coaching** | Session data â†’ Snowflake Cortex (Mistral/Llama/Mixtral) â†’ personalized feedback |

Together, this supports **multi-device teaching and learning** with visual, audio, and haptic feedback channels.